define({ entries : {
    "Liu2024Arondight": {
        "abstract": "Proposes Arondight, a red-teaming framework for jailbreaking large vision-language models (VLMs) using auto-generated multi-modal prompts via reinforcement learning.",
        "author": "Liu, Yi et al.",
        "doi": "10.1145/3664647.3681379",
        "journal": "ACM International Conference on Multimedia",
        "keywords": "attack_type:RL-based, modality:multimodal, setup:black-box, category:attack, year:2024, category_type:Reinforcement Learning & Prompt Search",
        "number": "1",
        "publisher": "ACM",
        "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated Multi-modal Jailbreak Prompts",
        "type": "article",
        "url": "https://dl.acm.org/doi/10.1145/3664647.3681379",
        "volume": "2024",
        "year": "2024"
    },
    "Qi2024VisualAdversarial": {
        "abstract": "Presents a universal visual adversarial example that bypasses safety filters in aligned LLMs with visual inputs.",
        "author": "Qi, Xiangyu et al.",
        "doi": "10.1609/aaai.v38i19.30150",
        "journal": "AAAI Conference on Artificial Intelligence",
        "keywords": "attack_type:optimization, modality:vision, setup:black-box, category:attack, year:2024, category_type:Optimization-Based Attacks",
        "number": "7",
        "publisher": "AAAI",
        "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
        "type": "article",
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/30150/32038",
        "volume": "38",
        "year": "2024"
    },
    "Huang2025PGJ": {
        "abstract": "Introduces a perception-guided jailbreak method that substitutes unsafe words with perceptually similar alternatives.",
        "author": "Huang, Yihao et al.",
        "doi": "10.1609/aaai.v39i25.34821",
        "journal": "AAAI Conference on Artificial Intelligence",
        "keywords": "attack_type:generation, modality:text-to-image, setup:black-box, category:attack, year:2025, category_type:Generation-Based Attacks",
        "number": "7",
        "publisher": "AAAI",
        "title": "Perception-Guided Jailbreak Against Text-to-Image Models",
        "type": "article",
        "url": "https://www.researchgate.net/publication/390721176_Perception-Guided_Jailbreak_Against_Text-to-Image_Models",
        "volume": "39",
        "year": "2025"
    },
    "Gong2025FigStep": {
        "abstract": "Proposes a jailbreak attack using typographic prompts to bypass text-based safety filters in VLMs.",
        "author": "Gong, Yichen et al.",
        "doi": "10.1609/aaai.v39i22.34568",
        "journal": "AAAI Conference on Artificial Intelligence",
        "keywords": "attack_type:generation, modality:vision, setup:black-box, category:attack, year:2025, category_type:Generation-Based Attacks",
        "number": "7",
        "publisher": "AAAI",
        "title": "FigStep: Jailbreaking Large Vision-Language Models via Typographic Visual Prompts",
        "type": "article",
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34568/36723",
        "volume": "39",
        "year": "2025"
    },
    "Wang2024WhiteBoxJailbreaks": {
        "abstract": "Presents a white-box multimodal jailbreak attack by co-optimizing image and text triggers to create universal attack inputs.",
        "author": "Wang, Ruofan et al.",
        "doi": "10.1145/3664647.3681092",
        "journal": "ACM International Conference on Multimedia",
        "keywords": "attack_type:optimization, modality:multimodal, setup:white-box, category:attack, year:2024, category_type:Optimization-Based Attacks",
        "number": "1",
        "publisher": "ACM",
        "title": "White-box Multimodal Jailbreaks Against Large Vision-Language Models",
        "type": "article",
        "url": "https://www.researchgate.net/publication/385306642_White-box_Multimodal_Jailbreaks_Against_Large_Vision-Language_Models",
        "volume": "2024",
        "year": "2024"
    },
    "Ma2024ColJailBreak": {
        "abstract": "Introduces ColJailBreak, a two-phase framework for generating and editing images to bypass safety filters in T2I models.",
        "author": "Ma, Yizhuo et al.",
        "doi": "",
        "journal": "NeurIPS 2024 (accepted)",
        "keywords": "attack_type:generation, modality:vision, setup:black-box, category:attack, year:2024, category_type:Generation-Based Attacks",
        "number": "1",
        "publisher": "NeurIPS",
        "title": "ColJailBreak: Collaborative Generation and Editing for Jailbreaking Text-to-Image Deep Generation",
        "type": "article",
        "url": "https://proceedings.neurips.cc/paper_files/paper/2024/file/6f11132f6ecbbcafafdf6decfc98f7be-Paper-Conference.pdf",
        "volume": "2024",
        "year": "2024"
    },
    "Ma2025JPA": {
        "abstract": "Proposes JPA, a black-box adversarial prompt attack that modifies text embeddings to control the emergence of harmful concepts.",
        "author": "Ma, Jiachen et al.",
        "doi": "",
        "journal": "Findings of NAACL 2025",
        "keywords": "attack_type:optimization, modality:text-to-image, setup:black-box, category:attack, year:2025, category_type:Optimization-Based Attacks",
        "number": "172",
        "publisher": "ACL",
        "title": "Jailbreaking Prompt Attack: A Controllable Adversarial Attack against Diffusion Models",
        "type": "article",
        "url": "https://aclanthology.org/2025.findings-naacl.172.pdf",
        "volume": "Findings",
        "year": "2025"
    },
    "Yang2024SneakyPrompt": {
        "abstract": "SneakyPrompt introduces an RL-based prompt modification framework to evade safety filters in T2I models while preserving semantics.",
        "author": "Yang, Yuchen et al.",
        "doi": "10.1109/SP54263.2024.00123",
        "journal": "IEEE Symposium on Security and Privacy (S&P)",
        "keywords": "attack_type:RL-based, modality:text-to-image, setup:black-box, category:attack, year:2024, category_type:Reinforcement Learning & Prompt Search",
        "number": "1",
        "publisher": "IEEE",
        "title": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
        "type": "article",
        "url": "https://www.computer.org/csdl/proceedings-article/sp/2024/313000a123/1Ub23wEASRO",
        "volume": "2024",
        "year": "2024"
    },
    "Shayegani2024JailbreakInPieces": {
        "abstract": "Proposes a compositional embedding-space adversarial attack on VLMs by pairing benign prompts with malicious images.",
        "author": "Shayegani, Erfan et al.",
        "doi": "https://iclr.cc/virtual/2024/poster/17767",
        "journal": "ICLR 2024 (Spotlight)",
        "keywords": "attack_type:optimization, modality:multimodal, setup:black-box, category:attack, year:2024, category_type:Optimization-Based Attacks",
        "number": "6613",
        "publisher": "ICLR",
        "title": "Jailbreak in Pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
        "type": "article",
        "url": "https://openreview.net/pdf?id=plmBsXHxgR",
        "volume": "2024",
        "year": "2024"
    },
    "Ding2023AlignIsNotEnough": {
        "abstract": "Demonstrates that aligned multimodal models can still be jailbroken using carefully constructed image-text combinations.",
        "author": "Ding, Yifu et al.",
        "doi": "https://doi.org/10.1109/TCSVT.2025.3526248",
        "journal": "IEEE",
        "keywords": "attack_type:generation, modality:multimodal, setup:black-box, category:attack, year:2023, category_type:Generation-Based Attacks",
        "number": "1",
        "publisher": "IEEE",
        "title": "Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models",
        "type": "article",
        "url": "https://ieeexplore.ieee.org/document/10829683",
        "volume": "2023",
        "year": "2023"
    },
    "Weng2025MMJBench": {
        "abstract": "Presents MMJ-Bench, a unified benchmark for evaluating jailbreak attacks and defenses in VLMs, covering six models and ten techniques.",
        "author": "Weng, Fenghua et al.",
        "doi": "https://doi.org/10.1609/aaai.v39i26.34983",
        "journal": "AAAI 2025",
        "keywords": "category:survey, benchmark, evaluation, defenses, attack categorization, VLMs, year:2025",
        "number": "39050",
        "publisher": "AAAI",
        "title": "MMJ-Bench: A Comprehensive Study on Jailbreak Attacks and Defenses for Vision Language Models",
        "type": "article",
        "url": "https://ojs.aaai.org/index.php/AAAI/article/view/34983/37138",
        "volume": "2025",
        "year": "2025"
    }
}});
